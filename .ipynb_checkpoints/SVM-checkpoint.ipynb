{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2c4920e-7c85-4d50-af36-bfe374cf9ea9",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e66875-240c-430d-b3da-5dfc4c3c9441",
   "metadata": {},
   "source": [
    "SVM offers very high accuracy compared to other classifiers such as logistic regression, and decision trees. It is known for its kernel trick to handle nonlinear input spaces. It is used in a variety of applications such as face detection, intrusion detection, classification of emails, news articles and web pages, classification of genes, and handwriting recognition. SVM is an exciting algorithm and the concepts are relatively simple. The classifier separates data points using a hyperplane with the largest amount of margin. That's why an SVM classifier is also known as a discriminative classifier. SVM finds an optimal hyperplane which helps in classifying new data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76dca88-03cb-473f-a396-ecf0278f5d5e",
   "metadata": {},
   "source": [
    "Generally, Support Vector Machines is considered to be a classification approach, it but can be employed in both types of classification and regression problems. It can easily handle multiple continuous and categorical variables. SVM constructs a hyperplane in multidimensional space to separate different classes. SVM generates optimal hyperplane in an iterative manner, which is used to minimize an error. The core idea of SVM is to find a **maximum marginal hyperplane(MMH)** that best divides the dataset into classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6266d722-16dc-40ff-accc-ce2afb12bcdc",
   "metadata": {},
   "source": [
    "![machinelearning](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1526288453/index3_souoaz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b92dcc-084e-490a-a813-b14834b890a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Stability Vectors\n",
    "The data points that are closest to the hyperplane are called support vectors. By computing margins, these points will better define the dividing line. These ideas are more crucial to the classifier's design.\n",
    "\n",
    "#### Hyperplane \n",
    "A hyperplane is a decision plane that divides a group of items into those belonging to several classes.\n",
    "\n",
    "#### Margin\n",
    "The distance between the two lines on the nearest class points is known as a margin. This is calculated as the angle between the line and the nearest points or support vectors. A wider gap between the classes is seen as a good gap; a smaller gap is regarded as a bad gap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58d3269-35a9-485c-a4fb-af98db55e296",
   "metadata": {},
   "source": [
    "### How does SVM work?\n",
    "The main objective is to segregate the given dataset in the best possible way. The distance between the either nearest points is known as the margin. The objective is to select a hyperplane with the maximum possible margin between support vectors in the given dataset. SVM searches for the maximum marginal hyperplane in the following steps:\n",
    "\n",
    "<ol>\n",
    "<li>Generate hyperplanes which segregates the classes in the best way. Left-hand side figure showing three hyperplanes black, blue and orange. Here, the blue and orange have higher classification error, but the black is separating the two classes correctly.</li>\n",
    "<li>\n",
    "Select the right hyperplane with the maximum segregation from the either nearest data points as shown in the right-hand side figure.\n",
    "</li>\n",
    "</ol>\n",
    "\n",
    "![machinelearning](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1526288454/index2_ub1uzd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f9bae2-dd2d-4ed8-b9ff-87b1fff38449",
   "metadata": {},
   "source": [
    "#### Dealing with non-linear and inseparable planes\n",
    "\n",
    "Some problems can’t be solved using linear hyperplane, as shown in the figure below (left-hand side).\n",
    "\n",
    "In such situation, SVM uses a kernel trick to transform the input space to a higher dimensional space as shown on the right. The data points are plotted on the x-axis and z-axis (Z is the squared sum of both x and y: $z=x^2+y^2$). Now you can easily segregate these points using linear separation.\n",
    "![machinelearning](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1526288453/index_bnr4rx.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc1a019-e8c1-4c45-b2c3-7b7f6ff811b9",
   "metadata": {},
   "source": [
    "## SVM Kernels\n",
    "\n",
    "The SVM algorithm is implemented in practice using a kernel. A kernel transforms an input data space into the required form. SVM uses a technique called the kernel trick. Here, the kernel takes a low-dimensional input space and transforms it into a higher dimensional space. In other words, you can say that it converts nonseparable problem to separable problems by adding more dimension to it. It is most useful in non-linear separation problem. Kernel trick helps you to build a more accurate classifier.\n",
    "\n",
    "**Linear Kernel:** A linear kernel can be used as normal dot product any two given observations. The product between two vectors is the sum of the multiplication of each pair of input values.\n",
    " $K(x, x_i) = \\sum {x \\times x_i}$\n",
    "\n",
    "**Polynomial Kernel:** A polynomial kernel is a more generalized form of the linear kernel. The polynomial kernel can distinguish curved or nonlinear input space.\n",
    "$ K(x, x_i) = 1 + \\sum{x \\times x_i}^d $\n",
    "\n",
    "Where $d$ is the degree of the polynomial. $d=1$ is similar to the linear transformation. The degree needs to be manually specified in the learning algorithm.\n",
    "\n",
    "**Radial Basis Function Kernel:** The Radial basis function kernel is a popular kernel function commonly used in support vector machine classification. RBF can map an input space in infinite dimensional space.\n",
    "$ K(x,x_i) = e^{-\\gamma * \\sum{x – x_i^2}} $\n",
    "\n",
    "Here gamma is a parameter, which ranges from 0 to 1. A higher value of gamma will perfectly fit the training dataset, which causes over-fitting. Gamma=0.1 is considered to be a good default value. The value of gamma needs to be manually specified in the learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58287bc5-c869-4344-b35b-f6017f2a685b",
   "metadata": {},
   "source": [
    "## Classifier Building in Scikit-learn\n",
    "\n",
    "You have so far studied about the theoretical foundation of SVM. You will now discover how it was implemented in Python using scikit-learn.\n",
    "\n",
    "We will use the cancer dataset, a well-known multi-class classification problem, in the model-building phase. This dataset was generated from a digitised image of a breast mass that was sampled with a fine needle aspirator (FNA). They characterise the traits of the visible cell nuclei in the picture.\n",
    "\n",
    "The dataset consists of 30 features, including mean radius, mean texture, mean perimeter, mean area, mean smoothness, mean compactness, mean concavity, mean concave points, mean symmetry, mean fractal dimension, radius error, mean texture error, mean area error, mean smoothness error, mean compactness error, mean concavity error, mean concave points error, mean fractal dimension error, and mean worst radius, worst texture, worst perimeter, worst area, worst smoothness, worst compact (type of cancer).\n",
    "\n",
    "This information includes two categories of cancer: benign and malignant (harmful). (not harmful). Here, you can create a model to categorise the many cancer types. The dataset can be downloaded from the UCI Machine Learning Library or from the scikit-learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4795b89-b160-45d4-9559-c2046400a5f3",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "Let's first load the required dataset you will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "718db1f8-9233-4390-a4f0-a8ed9d515594",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import scikit-learn dataset library\n",
    "from sklearn import datasets\n",
    "\n",
    "#Load dataset\n",
    "cancer = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2f8cb1-9add-47fc-b22b-abb09fc51f85",
   "metadata": {},
   "source": [
    "## Exploring Data\n",
    "After you have loaded the dataset, you might want to know a little bit more about it. You can check feature and target names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9975bfb-e214-4359-b61f-0a044093fc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
      "Labels:  ['malignant' 'benign']\n"
     ]
    }
   ],
   "source": [
    "# print the names of the 13 features\n",
    "print(\"Features: \", cancer.feature_names)\n",
    "\n",
    "# print the label type of cancer('malignant' 'benign')\n",
    "print(\"Labels: \", cancer.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f1ab318-8d4f-4c80-89ff-1e2b11dbe7e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print data(feature)shape\n",
    "cancer.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b4c0c96-04b7-432f-b5ca-d6ea2573fb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.799e+01 1.038e+01 1.228e+02 1.001e+03 1.184e-01 2.776e-01 3.001e-01\n",
      "  1.471e-01 2.419e-01 7.871e-02 1.095e+00 9.053e-01 8.589e+00 1.534e+02\n",
      "  6.399e-03 4.904e-02 5.373e-02 1.587e-02 3.003e-02 6.193e-03 2.538e+01\n",
      "  1.733e+01 1.846e+02 2.019e+03 1.622e-01 6.656e-01 7.119e-01 2.654e-01\n",
      "  4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 1.326e+03 8.474e-02 7.864e-02 8.690e-02\n",
      "  7.017e-02 1.812e-01 5.667e-02 5.435e-01 7.339e-01 3.398e+00 7.408e+01\n",
      "  5.225e-03 1.308e-02 1.860e-02 1.340e-02 1.389e-02 3.532e-03 2.499e+01\n",
      "  2.341e+01 1.588e+02 1.956e+03 1.238e-01 1.866e-01 2.416e-01 1.860e-01\n",
      "  2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 1.203e+03 1.096e-01 1.599e-01 1.974e-01\n",
      "  1.279e-01 2.069e-01 5.999e-02 7.456e-01 7.869e-01 4.585e+00 9.403e+01\n",
      "  6.150e-03 4.006e-02 3.832e-02 2.058e-02 2.250e-02 4.571e-03 2.357e+01\n",
      "  2.553e+01 1.525e+02 1.709e+03 1.444e-01 4.245e-01 4.504e-01 2.430e-01\n",
      "  3.613e-01 8.758e-02]\n",
      " [1.142e+01 2.038e+01 7.758e+01 3.861e+02 1.425e-01 2.839e-01 2.414e-01\n",
      "  1.052e-01 2.597e-01 9.744e-02 4.956e-01 1.156e+00 3.445e+00 2.723e+01\n",
      "  9.110e-03 7.458e-02 5.661e-02 1.867e-02 5.963e-02 9.208e-03 1.491e+01\n",
      "  2.650e+01 9.887e+01 5.677e+02 2.098e-01 8.663e-01 6.869e-01 2.575e-01\n",
      "  6.638e-01 1.730e-01]\n",
      " [2.029e+01 1.434e+01 1.351e+02 1.297e+03 1.003e-01 1.328e-01 1.980e-01\n",
      "  1.043e-01 1.809e-01 5.883e-02 7.572e-01 7.813e-01 5.438e+00 9.444e+01\n",
      "  1.149e-02 2.461e-02 5.688e-02 1.885e-02 1.756e-02 5.115e-03 2.254e+01\n",
      "  1.667e+01 1.522e+02 1.575e+03 1.374e-01 2.050e-01 4.000e-01 1.625e-01\n",
      "  2.364e-01 7.678e-02]]\n"
     ]
    }
   ],
   "source": [
    "# print the cancer data features (top 5 records)\n",
    "print(cancer.data[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08a4a5d3-dec2-4355-864a-a588dab3bccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# print the cancer labels (0:malignant, 1:benign)\n",
    "print(cancer.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6657044-c049-4b32-80f0-41e83fdeaa76",
   "metadata": {},
   "source": [
    "Splitting Data\n",
    "To understand model performance, dividing the dataset into a training set and a test set is a good strategy.\n",
    "\n",
    "Split the dataset by using the function train_test_split(). you need to pass 3 parameters features, target, and test_set size. Additionally, you can use random_state to select records randomly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
